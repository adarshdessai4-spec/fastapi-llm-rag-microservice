FastAPI LLM-RAG Microservice

A production-grade Python backend service demonstrating how to design, build, and deploy
LLM-powered APIs using FastAPI with real-world engineering practices.

This project focuses on productionizing AI systems â€” not just calling an LLM, but building
backend services that are reliable, observable, scalable, and safe.

ğŸš€ What this project demonstrates

âœ… Clean FastAPI architecture (routers / services / schemas / dependencies)
âœ… RESTful API design with validation and versioning
âœ… Retrieval-Augmented Generation (RAG)
âœ… Vector search using FAISS
âœ… Pluggable LLM layer (OpenAI or local stub)
âœ… Grounded responses with citations
âœ… Rate limiting and caching
âœ… Async I/O and performance awareness
âœ… Correlation IDs for observability
âœ… Dockerized deployment
âœ… CI pipeline with automated tests

Built as a portfolio-grade backend system for Senior Backend / Backend Engineer III (Python | AI/LLM) roles.

ğŸ¯ Why this project exists

Most AI demos stop at simply sending a prompt to an LLM.

In real backend systems, teams must handle:

hallucinations and unreliable responses

performance bottlenecks

API cost control

safe usage of external context

debugging and observability

reproducible deployments

This project demonstrates how LLM systems are actually built in production, not just how they are queried.

ğŸ§  High-Level Architecture
Client
  â”‚
  â–¼
FastAPI API Layer
  â”‚
  â”œâ”€â”€ Request validation (Pydantic)
  â”œâ”€â”€ Rate limiting
  â”œâ”€â”€ Correlation IDs
  â”‚
  â”œâ”€â”€ Ingestion Service
  â”‚       â”œâ”€â”€ Text / URL ingestion
  â”‚       â”œâ”€â”€ Chunking
  â”‚       â”œâ”€â”€ Embedding generation
  â”‚       â””â”€â”€ FAISS vector store
  â”‚
  â””â”€â”€ Chat Service
          â”œâ”€â”€ Semantic retrieval (top-k)
          â”œâ”€â”€ Prompt orchestration
          â”œâ”€â”€ LLM execution
          â””â”€â”€ Grounded response + citations

âš™ï¸ Tech Stack

Language: Python 3.11+

Framework: FastAPI

Validation: Pydantic v2

Vector Database: FAISS

LLM: OpenAI (optional) / Local stub

Async HTTP: httpx

Caching: TTL cache

CI: GitHub Actions

Containerization: Docker

ğŸš€ Quickstart (Local)
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload


Swagger UI:

http://127.0.0.1:8000/docs


Health check:

http://127.0.0.1:8000/health

âš¡ Demo (30 seconds)
1ï¸âƒ£ Ingest a document
curl -X POST http://127.0.0.1:8000/v1/ingest/text \
  -H "Content-Type: application/json" \
  -d '{"doc_id":"doc1","text":"FastAPI is a modern Python web framework for building APIs. It supports type hints, Pydantic validation, and automatic OpenAPI documentation."}'

2ï¸âƒ£ Ask a question (RAG)
curl -X POST http://127.0.0.1:8000/v1/chat/answer \
  -H "Content-Type: application/json" \
  -d '{"question":"What is FastAPI?"}'

Response includes

AI-generated answer

source citations from retrieved context

grounded output to reduce hallucination

ğŸ”Œ Core APIs
Method	Endpoint	Description
GET	/health	Service health
POST	/v1/ingest/text	Ingest raw text
POST	/v1/ingest/url	Ingest webpage
POST	/v1/chat/answer	RAG-based question answering
GET	/v1/admin/stats	Vector index statistics
ğŸ” Reliability & Safety

Per-IP request rate limiting

External call timeouts

LLM context isolation

â€œI donâ€™t knowâ€ fallback when context is missing

Citation-based grounding to reduce hallucinations

ğŸ§  Engineering Decisions

RAG instead of raw prompting to reduce hallucination

FAISS chosen for fast local vector search

Pluggable LLM interface to avoid vendor lock-in

Stub LLM for local/offline development

Service layer separation for clean architecture

Async I/O for I/O-bound workloads

Correlation IDs for request tracing and debugging

ğŸ³ Docker
docker build -t fastapi-llm-rag .
docker run -p 8000:8000 fastapi-llm-rag

ğŸ§ª Tests
pytest -q


Includes:

health endpoint test

ingestion + chat integration test

âœ… What this project demonstrates

End-to-end backend ownership

Strong Python engineering fundamentals

Real-world LLM system design

Production-first mindset

Clean, scalable FastAPI architecture

This mirrors how AI-enabled backend systems are built in real production environments.

ğŸ‘¤ Author

Adarsh Dessai
Senior Python Backend Engineer
Focus: FastAPI â€¢ Distributed Systems â€¢ AI / LLM Integration

âœ… Final command
git add README.md
git commit -m "Finalize production-grade README"
git push